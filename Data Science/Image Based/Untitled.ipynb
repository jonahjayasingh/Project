{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ec2416e-aa2b-4603-af37-59c22fcb5215",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found classes: ['Acne And Rosacea Photos', 'Malignant', 'Fu Athlete Foot', 'Eczema Photos', 'Fu Ringworm', 'Benign', 'Exanthems And Drug Eruptions', 'Lupus And Other Connective Tissue Diseases', 'Ba Impetigo', 'Bullous Disease Photos', 'Nail Fungus And Other Nail Disease', 'Light Diseases And Disorders Of Pigmentation', 'Atopic Dermatitis Photos', 'Actinic Keratosis Basal Cell Carcinoma And Other Malignant Lesions', 'Melanoma Skin Cancer Nevi And Moles', 'Cellulitis Impetigo And Other Bacterial Infections', 'Hair Loss Photos Alopecia And Other Hair Diseases', 'Herpes Hpv And Other Stds Photos', 'Fu Nail Fungus', 'Ba  Cellulitis']\n",
      "Total classes: 20\n",
      "Class 'Acne And Rosacea Photos' has 6837 images\n",
      "Class 'Malignant' has 6762 images\n",
      "Class 'Fu Athlete Foot' has 8054 images\n",
      "Class 'Eczema Photos' has 6715 images\n",
      "Class 'Fu Ringworm' has 8129 images\n",
      "Class 'Benign' has 6459 images\n",
      "Class 'Exanthems And Drug Eruptions' has 7750 images\n",
      "Class 'Lupus And Other Connective Tissue Diseases' has 7734 images\n",
      "Class 'Ba Impetigo' has 8148 images\n",
      "Class 'Bullous Disease Photos' has 7695 images\n",
      "Class 'Nail Fungus And Other Nail Disease' has 6956 images\n",
      "Class 'Light Diseases And Disorders Of Pigmentation' has 7546 images\n",
      "Class 'Atopic Dermatitis Photos' has 7645 images\n",
      "Class 'Actinic Keratosis Basal Cell Carcinoma And Other Malignant Lesions' has 6821 images\n",
      "Class 'Melanoma Skin Cancer Nevi And Moles' has 7680 images\n",
      "Class 'Cellulitis Impetigo And Other Bacterial Infections' has 7894 images\n",
      "Class 'Hair Loss Photos Alopecia And Other Hair Diseases' has 7955 images\n",
      "Class 'Herpes Hpv And Other Stds Photos' has 7744 images\n",
      "Class 'Fu Nail Fungus' has 8083 images\n",
      "Class 'Ba  Cellulitis' has 8079 images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# Path to your dataset folder\n",
    "# Each class should be in a separate folder inside this directory\n",
    "dataset_path = \"dataset\"\n",
    "\n",
    "# List all subfolders (classes)\n",
    "classes = [d for d in os.listdir(dataset_path) if os.path.isdir(os.path.join(dataset_path, d))]\n",
    "\n",
    "print(\"Found classes:\", classes)\n",
    "print(\"Total classes:\", len(classes))\n",
    "# Count images in each class\n",
    "for cls in classes:\n",
    "    class_path = os.path.join(dataset_path, cls)\n",
    "    # Assuming common image extensions\n",
    "    image_files = glob.glob(os.path.join(class_path, \"*.[pjPJ]*[ngNG]*\"))  # jpg, jpeg, png\n",
    "    print(f\"Class '{cls}' has {len(image_files)} images\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10cb9ff1-4eea-436b-a74a-61c0f33dcb17",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy(\"mixed_float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d777748-5af5-44d7-89de-43a2f0e33b60",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Suppress TensorFlow Python logs\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "\n",
    "# Suppress ML Compute / Metal debug info\n",
    "os.environ[\"TF_METAL_ENABLE_LOGGING\"] = \"0\"\n",
    "os.environ[\"TF_GPU_THREAD_MODE\"] = \"gpu_private\"\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.debugging.set_log_device_placement(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2586a3af-a221-42f7-955f-b47831b15bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "205d54d5-8cae-4ea5-a838-ad92d8d0bb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT = \"dataset\"   # <-- change to your dataset folder\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 15\n",
    "VAL_SPLIT = 0.15\n",
    "SEED = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d61dd488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 150686 files belonging to 20 classes.\n",
      "Using 128084 files for training.\n",
      "Found 150686 files belonging to 20 classes.\n",
      "Using 22602 files for validation.\n",
      "Classes: 20\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_ROOT,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATA_ROOT,\n",
    "    validation_split=VAL_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=SEED,\n",
    "    image_size=(IMG_SIZE, IMG_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    ")\n",
    "\n",
    "class_names = train_ds.class_names\n",
    "num_classes = len(class_names)\n",
    "print(f\"Classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52c54e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds   = val_ds.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d04ce41",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal\"),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3c1f005b",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"weights_mobilenet_v3_large_224_1.0_float_no_top_v2.h5\",\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    pooling=\"avg\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "467dae15",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False  # freeze for transfer learning\n",
    "\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = keras.applications.efficientnet.preprocess_input(x)\n",
    "x = base_model(x, training=False)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67e41afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2bbed8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=3e-4),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de56da41",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(factor=0.5, patience=2),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "061c44d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2459s\u001b[0m 608ms/step - accuracy: 0.5545 - loss: 1.4807 - val_accuracy: 0.6543 - val_loss: 1.1290 - learning_rate: 3.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2413s\u001b[0m 597ms/step - accuracy: 0.6361 - loss: 1.1659 - val_accuracy: 0.6763 - val_loss: 1.0278 - learning_rate: 3.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2408s\u001b[0m 594ms/step - accuracy: 0.6526 - loss: 1.0976 - val_accuracy: 0.6974 - val_loss: 0.9741 - learning_rate: 3.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2396s\u001b[0m 592ms/step - accuracy: 0.6620 - loss: 1.0606 - val_accuracy: 0.7052 - val_loss: 0.9383 - learning_rate: 3.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2385s\u001b[0m 589ms/step - accuracy: 0.6650 - loss: 1.0432 - val_accuracy: 0.7103 - val_loss: 0.9169 - learning_rate: 3.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2374s\u001b[0m 586ms/step - accuracy: 0.6697 - loss: 1.0279 - val_accuracy: 0.7137 - val_loss: 0.9021 - learning_rate: 3.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2387s\u001b[0m 590ms/step - accuracy: 0.6723 - loss: 1.0186 - val_accuracy: 0.7180 - val_loss: 0.8892 - learning_rate: 3.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2380s\u001b[0m 588ms/step - accuracy: 0.6746 - loss: 1.0106 - val_accuracy: 0.7184 - val_loss: 0.8822 - learning_rate: 3.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2293s\u001b[0m 566ms/step - accuracy: 0.6739 - loss: 1.0057 - val_accuracy: 0.7202 - val_loss: 0.8737 - learning_rate: 3.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2020s\u001b[0m 500ms/step - accuracy: 0.6736 - loss: 1.0027 - val_accuracy: 0.7220 - val_loss: 0.8695 - learning_rate: 3.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2034s\u001b[0m 502ms/step - accuracy: 0.6756 - loss: 1.0011 - val_accuracy: 0.7252 - val_loss: 0.8592 - learning_rate: 3.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2374s\u001b[0m 588ms/step - accuracy: 0.6749 - loss: 1.0002 - val_accuracy: 0.7218 - val_loss: 0.8649 - learning_rate: 3.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2349s\u001b[0m 580ms/step - accuracy: 0.6763 - loss: 0.9961 - val_accuracy: 0.7242 - val_loss: 0.8566 - learning_rate: 3.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2019s\u001b[0m 500ms/step - accuracy: 0.6782 - loss: 0.9903 - val_accuracy: 0.7270 - val_loss: 0.8534 - learning_rate: 3.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2032s\u001b[0m 502ms/step - accuracy: 0.6763 - loss: 0.9932 - val_accuracy: 0.7293 - val_loss: 0.8458 - learning_rate: 3.0000e-04\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58908f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15234s\u001b[0m 4s/step - accuracy: 0.5478 - loss: 1.4910 - val_accuracy: 0.6746 - val_loss: 1.0025 - learning_rate: 1.0000e-05\n",
      "Epoch 2/5\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13007s\u001b[0m 3s/step - accuracy: 0.6793 - loss: 0.9879 - val_accuracy: 0.7359 - val_loss: 0.8214 - learning_rate: 1.0000e-05\n",
      "Epoch 3/5\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15136s\u001b[0m 4s/step - accuracy: 0.7356 - loss: 0.8125 - val_accuracy: 0.7744 - val_loss: 0.7046 - learning_rate: 1.0000e-05\n",
      "Epoch 4/5\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18158s\u001b[0m 5s/step - accuracy: 0.7761 - loss: 0.6863 - val_accuracy: 0.7987 - val_loss: 0.6200 - learning_rate: 1.0000e-05\n",
      "Epoch 5/5\n",
      "\u001b[1m4003/4003\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17254s\u001b[0m 4s/step - accuracy: 0.8073 - loss: 0.5867 - val_accuracy: 0.8284 - val_loss: 0.5334 - learning_rate: 1.0000e-05\n"
     ]
    }
   ],
   "source": [
    "base_model.trainable = True\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(1e-5),\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "fine_history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=5,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "32b377e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save(\"skin_disease_model_tf.h5\")\n",
    "model.save(\"skin_disease_model_tf.keras\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
