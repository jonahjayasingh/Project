{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f8d8ec97-bbc0-4a27-b16a-0a7b752db793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "Found classes: ['Acne And Rosacea Photos', 'Actinic Keratosis Basal Cell Carcinoma And Other Malignant Lesions', 'Atopic Dermatitis Photos', 'Ba  Cellulitis', 'Ba Impetigo', 'Benign', 'Bullous Disease Photos', 'Cellulitis Impetigo And Other Bacterial Infections', 'Eczema Photos', 'Exanthems And Drug Eruptions', 'Fu Athlete Foot', 'Fu Nail Fungus', 'Fu Ringworm', 'Hair Loss Photos Alopecia And Other Hair Diseases', 'Heathy', 'Herpes Hpv And Other Stds Photos', 'Light Diseases And Disorders Of Pigmentation', 'Lupus And Other Connective Tissue Diseases', 'Malignant', 'Melanoma Skin Cancer Nevi And Moles']\n",
      "Class 'Acne And Rosacea Photos' has 6837 images\n",
      "Class 'Actinic Keratosis Basal Cell Carcinoma And Other Malignant Lesions' has 6821 images\n",
      "Class 'Atopic Dermatitis Photos' has 7645 images\n",
      "Class 'Ba  Cellulitis' has 8079 images\n",
      "Class 'Ba Impetigo' has 8148 images\n",
      "Class 'Benign' has 6459 images\n",
      "Class 'Bullous Disease Photos' has 7695 images\n",
      "Class 'Cellulitis Impetigo And Other Bacterial Infections' has 7894 images\n",
      "Class 'Eczema Photos' has 6715 images\n",
      "Class 'Exanthems And Drug Eruptions' has 7750 images\n",
      "Class 'Fu Athlete Foot' has 8054 images\n",
      "Class 'Fu Nail Fungus' has 8083 images\n",
      "Class 'Fu Ringworm' has 8129 images\n",
      "Class 'Hair Loss Photos Alopecia And Other Hair Diseases' has 7955 images\n",
      "Class 'Heathy' has 7758 images\n",
      "Class 'Herpes Hpv And Other Stds Photos' has 7744 images\n",
      "Class 'Light Diseases And Disorders Of Pigmentation' has 7546 images\n",
      "Class 'Lupus And Other Connective Tissue Diseases' has 7734 images\n",
      "Class 'Malignant' has 6762 images\n",
      "Class 'Melanoma Skin Cancer Nevi And Moles' has 7680 images\n",
      "Total images: 151488\n",
      "Train images: 121190, Validation images: 30298\n",
      "Sample batch images shape: torch.Size([32, 3, 224, 224]), labels shape: torch.Size([32])\n",
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 98\u001b[0m\n\u001b[1;32m     95\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     97\u001b[0m _, preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 98\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     99\u001b[0m running_corrects \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(preds \u001b[38;5;241m==\u001b[39m labels)\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Update progress bar (float only, no double)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# --------------------------\n",
    "# 1. Device Setup\n",
    "# --------------------------\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --------------------------\n",
    "# 2. Dataset Paths & Augmentation\n",
    "# --------------------------\n",
    "data_dir = \"dataset\"  # folder containing 20 classes\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "full_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# --------------------------\n",
    "# 3. Print dataset info\n",
    "# --------------------------\n",
    "print(f\"Found classes: {full_dataset.classes}\")\n",
    "for idx, class_name in enumerate(full_dataset.classes):\n",
    "    class_count = sum(1 for item in full_dataset.targets if item == idx)\n",
    "    print(f\"Class '{class_name}' has {class_count} images\")\n",
    "print(f\"Total images: {len(full_dataset)}\")\n",
    "\n",
    "# --------------------------\n",
    "# 4. Train/Validation Split\n",
    "# --------------------------\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "print(f\"Train images: {train_size}, Validation images: {val_size}\")\n",
    "\n",
    "# --------------------------\n",
    "# 5. DataLoaders\n",
    "# --------------------------\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=0)\n",
    "\n",
    "# Test one batch to confirm loading\n",
    "images, labels = next(iter(train_loader))\n",
    "print(f\"Sample batch images shape: {images.shape}, labels shape: {labels.shape}\")\n",
    "\n",
    "# --------------------------\n",
    "# 6. Model (EfficientNet-B0)\n",
    "# --------------------------\n",
    "model = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.IMAGENET1K_V1)\n",
    "num_ftrs = model.classifier[1].in_features\n",
    "model.classifier[1] = nn.Linear(num_ftrs, len(full_dataset.classes))  # 20 classes\n",
    "model = model.to(device)\n",
    "\n",
    "# --------------------------\n",
    "# 7. Loss & Optimizer\n",
    "# --------------------------\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# --------------------------\n",
    "# 8. Training Loop\n",
    "# --------------------------\n",
    "epochs = 10\n",
    "best_acc = 0.0\n",
    "start_time = time.time()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    # --- Training ---\n",
    "    model.train()\n",
    "    running_loss, running_corrects = 0.0, 0\n",
    "    train_bar = tqdm(train_loader, desc=\"Training\", leave=False)\n",
    "    for inputs, labels in train_bar:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        # Update progress bar (float only, no double)\n",
    "        train_bar.set_postfix(loss=(running_loss / (train_bar.n + 1)),\n",
    "                              acc=(running_corrects.float().item() / (train_bar.n + 1)))\n",
    "\n",
    "    epoch_loss = running_loss / len(train_dataset)\n",
    "    epoch_acc = running_corrects.double() / len(train_dataset)\n",
    "    print(f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    val_loss, val_corrects = 0.0, 0\n",
    "    val_bar = tqdm(val_loader, desc=\"Validation\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_bar:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            val_corrects += torch.sum(preds == labels)\n",
    "\n",
    "            val_bar.set_postfix(loss=(val_loss / (val_bar.n + 1)),\n",
    "                                acc=(val_corrects.float().item() / (val_bar.n + 1)))\n",
    "\n",
    "    val_loss = val_loss / len(val_dataset)\n",
    "    val_acc = val_corrects.double() / len(val_dataset)\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        print(f\"Saved new best model with Val Acc: {best_acc:.4f}\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(f\"\\nTraining complete in {total_time/60:.2f} minutes\")\n",
    "print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99915a33-a465-4e7f-a70f-d03010f6835c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579d2fb6-723c-4fd4-96b1-e4f88a5951b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d47d19-c102-44f7-bd54-8587a45d3f60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4352c163-4d49-4fc4-a0d3-0a258053602b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
